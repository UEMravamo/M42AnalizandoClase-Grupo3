{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ac452767-0b51-4433-a2ac-f3414203af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MÉTODO 1 (perspectiva alumno)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, date_format, avg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a98cf9c8-cd3c-4b1a-8879-2f47d20c92fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/10 19:25:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Crear la sesión Spark\n",
    "spark = SparkSession.builder.appName(\"PrediccionAsistencia\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "63272aa2-9a87-49fe-89a7-32370f2f04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo CSV en PySpark\n",
    "file_path = \"asistencia_test.csv\"\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0a5a53a6-20e6-4fb3-a7ac-32949fd67fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones guardadas en: forecast_por_alumno.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spark_env/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "# Pasar estado a valor numérico\n",
    "data = data.withColumn(\n",
    "    \"estado_codificado\",\n",
    "    when(col(\"estado_asistencia\") == \"presente\", 1)\n",
    "    .when(col(\"estado_asistencia\") == \"tarde\", 0.5)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Agrupamos por alumno, asignatura y día y calcular estado promedio\n",
    "series_temporales = data.groupBy(\n",
    "    \"alumno\",\n",
    "    \"asignatura\",\n",
    "    date_format(\"timestamp\", \"yyyy-MM-dd\").alias(\"fecha\") # Pasar timestamp a fecha\n",
    ").agg(\n",
    "    avg(\"estado_codificado\").alias(\"estado_promedio\")  # Calcula media del estado\n",
    ")\n",
    "\n",
    "# Convertir el dataframe de PySpark a Pandas\n",
    "series_pandas = series_temporales.toPandas()\n",
    "\n",
    "# Ordenar los datos\n",
    "series_pandas['fecha'] = pd.to_datetime(series_pandas['fecha'])\n",
    "series_pandas = series_pandas.sort_values(by=['alumno', 'asignatura', 'fecha'])\n",
    "\n",
    "# Almacenar resultados de predicciones\n",
    "resultados_predicciones = []\n",
    "\n",
    "# ARIMA para cada alumno y asignatura\n",
    "for (alumno, asignatura), grupo in series_pandas.groupby([\"alumno\", \"asignatura\"]):\n",
    "    grupo = grupo.sort_values(\"fecha\")\n",
    "\n",
    "    # Ordenar la lista por fecha\n",
    "    grupo = grupo.set_index(\"fecha\")\n",
    "    grupo.index.freq = 'D'  # Frecuencia diaria\n",
    "\n",
    "    serie = grupo[\"estado_promedio\"]\n",
    "\n",
    "    # Solo entrenamos si hay suficientes datos\n",
    "    if len(serie) > 10:\n",
    "        try:\n",
    "            # Entrenamiento del modelo\n",
    "            modelo = ARIMA(serie, order=(1, 1, 0)) # p, d, q\n",
    "            ajuste = modelo.fit()\n",
    "\n",
    "            # Se predice el día siguiente\n",
    "            predicciones = ajuste.forecast(steps=1)\n",
    "            ultima_fecha = grupo.index[-1]\n",
    "\n",
    "            for i, pred in enumerate(predicciones):\n",
    "                 dia_predicho = ultima_fecha + timedelta(days=i+1)\n",
    "                 estado_predicho = pred\n",
    "                 estado_categoria = (\n",
    "                     \"presente\" if estado_predicho >= 0.75 else\n",
    "                     \"tarde\" if estado_predicho >= 0.25 else\n",
    "                     \"ausente\"\n",
    "                 )\n",
    "\n",
    "            resultados_predicciones.append({\n",
    "                    \"alumno\": alumno,\n",
    "                    \"asignatura\": asignatura,\n",
    "                    \"dia_predicho\": dia_predicho.strftime(\"%Y-%m-%d\"),\n",
    "                    \"estado_predicho_valor\": estado_predicho,\n",
    "                    \"estado_predicho_categoria\": estado_categoria\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error al ajustar ARIMA para {alumno}, {asignatura}: {e}\")\n",
    "\n",
    "# Convertimos los resultados a un df de Pandas\n",
    "df_resultados = pd.DataFrame(resultados_predicciones)\n",
    "\n",
    "# Exportación de los resultados a un CSV\n",
    "output_path = \"forecast_por_alumno.csv\"\n",
    "df_resultados.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Predicciones guardadas en: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d84f5a9b-182c-45d4-98b0-6fbb35536564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones globales guardadas en: forecast_por_asignatura1.csv\n"
     ]
    }
   ],
   "source": [
    "# MÉTODO 2 (por asignatura)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, date_format, avg, count\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Crear la sesión Spark\n",
    "spark = SparkSession.builder.appName(\"PrediccionAsistenciaGlobal\").getOrCreate()\n",
    "\n",
    "# Leer el archivo CSV en PySpark\n",
    "file_path = \"asistencia_test.csv\"\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Formatear las fechas\n",
    "data = data.withColumn(\"fecha\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Calcular totales diarios por asignatura y estado de asistencia\n",
    "totales_diarios = data.groupBy(\n",
    "    \"asignatura\", \"estado_asistencia\", \"fecha\"\n",
    ").agg(count(\"alumno\").alias(\"total_diario\"))\n",
    "\n",
    "# Convertir el DataFrame de PySpark a Pandas\n",
    "totales_pandas = totales_diarios.toPandas()\n",
    "\n",
    "# Ordenar y preparar las series temporales\n",
    "totales_pandas[\"fecha\"] = pd.to_datetime(totales_pandas[\"fecha\"])\n",
    "totales_pandas = totales_pandas.sort_values(by=[\"asignatura\", \"estado_asistencia\", \"fecha\"])\n",
    "\n",
    "# Almacenar resultados de predicciones\n",
    "resultados_forecast = []\n",
    "\n",
    "# Agrupar series temporales por asignatura y estado de asistencia\n",
    "series_agrupadas = totales_pandas.groupby([\"asignatura\", \"estado_asistencia\"])\n",
    "\n",
    "# Iterar sobre cada grupo (asignatura y estado de asistencia)\n",
    "for (asignatura, estado), grupo in series_agrupadas:\n",
    "    grupo = grupo.sort_values(\"fecha\")\n",
    "    grupo = grupo.set_index(\"fecha\")\n",
    "    grupo.index.freq = 'D'\n",
    "\n",
    "    # Asegurarse de que hay suficientes datos\n",
    "    if len(grupo) > 10:\n",
    "        serie = grupo[\"total_diario\"]\n",
    "        ultimo_dia = grupo.index[-1]\n",
    "\n",
    "        try:\n",
    "            # Entrenamiento del modelo\n",
    "            modelo = ARIMA(serie, order=(5, 1, 0)) # p, d, q\n",
    "            ajuste = modelo.fit()\n",
    "\n",
    "            # Se predice el día siguiente\n",
    "            prediccion = ajuste.forecast(steps=1)\n",
    "\n",
    "            # Calcular el día predicho\n",
    "            dia_predicho = ultimo_dia + timedelta(days=1)\n",
    "\n",
    "            # Almacenar el resultado\n",
    "            resultados_forecast.append({\n",
    "                \"asignatura\": asignatura,\n",
    "                \"dia\": dia_predicho.strftime(\"%Y-%m-%d\"),\n",
    "                \"estado_asistencia\": estado,\n",
    "                \"numero_alumnos\": round(prediccion.iloc[0])  # Redondear el valor\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error al ajustar ARIMA para {asignatura} - {estado}: {e}\")\n",
    "\n",
    "# Convertimos los resultados a un DataFrame de Pandas\n",
    "df_forecast = pd.DataFrame(resultados_forecast)\n",
    "\n",
    "# Exportación de los resultados a un CSV\n",
    "output_path = \"forecast_por_asignatura1.csv\"\n",
    "df_forecast.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Predicciones globales guardadas en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1b259-32b2-4866-87a9-3e8b4cfde463",
   "metadata": {},
   "source": [
    "## Funciones para probar el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "482ef72f-cf1b-4ceb-aa19-8e2953859b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprobar_num_columnas_spark(df):\n",
    "    resultado_esperado = 5\n",
    "    n_columnas = len(df.columns)\n",
    "    if n_columnas == resultado_esperado:\n",
    "        print(f\"OK -> El DataFrame de Spark tiene {n_columnas} columnas (lo esperado).\")\n",
    "    else:\n",
    "        print(f\"FAIL -> Se esperaban {resultado_esperado} columnas, pero tiene {n_columnas}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5723cde5-770b-4efa-9cd5-996fe796c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprobar_num_filas_spark(df):\n",
    "    min_rows = 2710\n",
    "    n_filas = df.count()\n",
    "    if n_filas >= min_rows:\n",
    "        print(f\"OK -> El DataFrame de Spark tiene {n_filas} filas, mínimo esperado era {min_rows}.\")\n",
    "    else:\n",
    "        print(f\"FAIL -> El DataFrame de Spark tiene {n_filas} filas, es menos de {min_rows}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ed5ef74a-f027-4828-899d-d6e6b2c49e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que se haga bien el cambio a datatime. Ya qué spark trabaja con Timestamp pero pandas trabaja con datetime para facilitar que se haga el trabajo de\n",
    "# ordenacion y la medición de fecuencia diaria línea:     grupo.index.freq = 'D'  # Frecuencia diaria\n",
    "def comprobar_columna_fecha_pandas(df):\n",
    "    columna_fecha = \"fecha\"\n",
    "    if pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        print(\"OK -> La columna 'fecha' en el DataFrame de Pandas es de tipo datetime.\")\n",
    "    else:\n",
    "        print(\"FAIL -> La columna 'fecha' NO es de tipo datetime.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6370ef7a-eb23-496b-b2f0-9d4d888f6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que el df se rellena bien\n",
    "def comprobar_resultados_no_vacios(df):\n",
    "    n_nulls = df.isnull().sum()\n",
    "    hay_nulls = False\n",
    "    \n",
    "    for col, num_nulls in n_nulls.items():\n",
    "        if num_nulls > 0:\n",
    "            print(f\"FAIL -> La columna '{col}' tiene {num_nulls} valores nulos.\")\n",
    "            hay_nulls = True\n",
    "        else:\n",
    "            print(f\"OK -> La columna '{col}' no tiene valores nulos.\")\n",
    "    \n",
    "    if not hay_nulls:\n",
    "        print(\"OK -> No se han encontrado nulos en ninguna columna.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9f8ab53e-e8b2-4e4f-85e4-b8c37aeee611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprobar_arima_alumnos(df_pandas):\n",
    "    columnas_esperadas = [\n",
    "        \"alumno\",\n",
    "        \"asignatura\",\n",
    "        \"dia_predicho\",\n",
    "        \"estado_predicho_valor\",\n",
    "        \"estado_predicho_categoria\"\n",
    "    ]\n",
    "        \n",
    "    for col in columnas_esperadas:\n",
    "        if col not in df_pandas.columns:\n",
    "            print(f\"FAIL -> Falta la columna '{col}' en los resultados de predicción.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"OK -> La columna '{col}' se encuentra en dentro de columnas_esperadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f4ebc9ed-49d5-46bb-a561-a9b2f67cc30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprobar_arima_asignaturas(df_pandas):\n",
    "    columnas_esperadas = [\n",
    "        \"asignatura\",\n",
    "        \"dia\",\n",
    "        \"estado_asistencia\",\n",
    "        \"numero_alumnos\"\n",
    "    ]\n",
    "        \n",
    "    for col in columnas_esperadas:\n",
    "        if col not in df_pandas.columns:\n",
    "            print(f\"FAIL -> Falta la columna '{col}' en los resultados de predicción.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"OK -> La columna '{col}' se encuentra en dentro de columnas_esperadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "35dda530-d188-4d8d-bd56-5e4a964f563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprobar_duplicados(df_pandas):\n",
    "\n",
    "    dfprueba = df_pandas[[\"alumno\", \"asignatura\", \"dia_predicho\", \"estado_predicho_valor\", \"estado_predicho_categoria\"]]\n",
    "    \n",
    "    num_filas = len(dfprueba) # numero de filas del df que acabamos de crear\n",
    "    num_filas_unicas = len(dfprueba.drop_duplicates())\n",
    "    \n",
    "    # Si num_filas_unicas == num_filas, significa que no se han quitado filas, por lo tanto no había duplicados.\n",
    "    if num_filas_unicas < num_filas:\n",
    "        print(\"FAIL -> Se encontraron filas duplicadas en el df.\")\n",
    "    else:\n",
    "        print(\"OK -> No hay duplicados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "813c9ba5-ccd6-49a0-8854-973d970f89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprobar_duplicados_asignaturas(df_pandas):\n",
    "\n",
    "    dfprueba = df_pandas[[\"asignatura\", \"dia\", \"estado_asistencia\", \"numero_alumnos\"]]\n",
    "    \n",
    "    num_filas = len(dfprueba) # numero de filas del df que acabamos de crear\n",
    "    num_filas_unicas = len(dfprueba.drop_duplicates())\n",
    "    \n",
    "    # Si num_filas_unicas == num_filas, significa que no se han quitado filas, por lo tanto no había duplicados.\n",
    "    if num_filas_unicas < num_filas:\n",
    "        print(\"FAIL -> Se encontraron filas duplicadas en el df.\")\n",
    "    else:\n",
    "        print(\"OK -> No hay duplicados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01109b5f-6fa2-4e7b-8851-ba1ee6d3da7d",
   "metadata": {},
   "source": [
    "## EJECUCIÓN DE PRUEBAS POR ALUMNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "956c2229-f4d0-4d11-9c6e-a391cf1c3601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> El DataFrame de Spark tiene 5 columnas (lo esperado).\n",
      "OK -> El DataFrame de Spark tiene 2710 filas, mínimo esperado era 2710.\n",
      "OK -> La columna 'fecha' en el DataFrame de Pandas es de tipo datetime.\n",
      "OK -> La columna 'alumno' no tiene valores nulos.\n",
      "OK -> La columna 'asignatura' no tiene valores nulos.\n",
      "OK -> La columna 'dia_predicho' no tiene valores nulos.\n",
      "OK -> La columna 'estado_predicho_valor' no tiene valores nulos.\n",
      "OK -> La columna 'estado_predicho_categoria' no tiene valores nulos.\n",
      "OK -> No se han encontrado nulos en ninguna columna.\n",
      "OK -> La columna 'alumno' se encuentra en dentro de columnas_esperadas.\n",
      "OK -> La columna 'asignatura' se encuentra en dentro de columnas_esperadas.\n",
      "OK -> La columna 'dia_predicho' se encuentra en dentro de columnas_esperadas.\n",
      "OK -> La columna 'estado_predicho_valor' se encuentra en dentro de columnas_esperadas.\n",
      "OK -> La columna 'estado_predicho_categoria' se encuentra en dentro de columnas_esperadas.\n",
      "OK -> No hay duplicados.\n"
     ]
    }
   ],
   "source": [
    "comprobar_num_columnas_spark(data)\n",
    "comprobar_num_filas_spark(data)\n",
    "comprobar_columna_fecha_pandas(series_pandas)\n",
    "comprobar_resultados_no_vacios(df_resultados)\n",
    "comprobar_arima_alumnos(df_resultados)\n",
    "comprobar_duplicados(df_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165e04b-7bcb-46b2-84da-8d44eb07c267",
   "metadata": {},
   "source": [
    "## EJECUCIÓN DE PRUEBAS POR ASIGNATURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "15e75cf8-f36d-43b2-b69e-e23c1683b669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> La columna 'asignatura' no tiene valores nulos.\n",
      "OK -> La columna 'dia' no tiene valores nulos.\n",
      "OK -> La columna 'estado_asistencia' no tiene valores nulos.\n",
      "OK -> La columna 'numero_alumnos' no tiene valores nulos.\n",
      "OK -> No se han encontrado nulos en ninguna columna.\n",
      "OK -> La columna 'asignatura' se encuentra en dentro de columnas_esperadas.\n",
      "OK -> La columna 'dia' se encuentra en dentro de columnas_esperadas.\n",
      "OK -> La columna 'estado_asistencia' se encuentra en dentro de columnas_esperadas.\n",
      "OK -> La columna 'numero_alumnos' se encuentra en dentro de columnas_esperadas.\n",
      "OK -> No hay duplicados.\n"
     ]
    }
   ],
   "source": [
    "comprobar_resultados_no_vacios(df_forecast)\n",
    "comprobar_arima_asignaturas(df_forecast)\n",
    "comprobar_duplicados_asignaturas(df_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33578483-2dae-4259-be4c-95858e579241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_env)",
   "language": "python",
   "name": "spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
